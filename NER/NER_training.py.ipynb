{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will attempt to do the Named Entity Recognition task. I will train two models; one is quick & reliable (Gold Parse), and the other is a deep-learning (word-embedding) model.\n",
    "\n",
    "The models are:<br>\n",
    "1- spaCy NER Gold Parse system [1]. spaCy is an industrial strength NLP library that is widely used by large corporation to handle similar NLP applications. E.g. it's being used by BBC, Microsoft, and many other big companies. spaCy is quick to train with reasonable accuracy, and generates a light model that can be ran quickly on a small machine.\n",
    "\n",
    "2- Flair embeddings [2]. Contextual String Embeddings for Sequence Labeling is currently the state-of-the-art [3] system in Named Entity Recognition tasks, and the only system outperforming Google's BERT [4] model. More information on Flair can be found on their paper [5]. Flair is an expensive to train model, however, it achieves state-of-the-art results.\n",
    "\n",
    "I have selected these two models to demonstrate that I am capable of providing a quick & reliable solution when needed (spaCy). Also, when time/resource allows, I am capable of providing a significantly better solution that is considered the state-of-the-art in the field of NLP (Flair Contextual String Embeddings).\n",
    "\n",
    "spaCy needs a couple of hours to train a decent model on a potato laptop, while Flair embeddings (with CharLMEmbeddings) can take days on the same laptop. Note, it took me less than an hour to fine-tune Flair with a big GPU Nvidia-Quadro-P6000. \n",
    "\n",
    "Requirements to run this code:\n",
    "- python 3.6\n",
    "- spacy '2.0.16'\n",
    "- flair\n",
    "\n",
    "[1] https://spacy.io/<br>\n",
    "[2] https://github.com/zalandoresearch/flair<br>\n",
    "[3] https://github.com/zalandoresearch/flair#comparison-with-state-of-the-art<br>\n",
    "[4] https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html<br>\n",
    "[5] https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy import displacy\n",
    "import json\n",
    "import random\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import SequenceTaggerTrainer\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, CharLMEmbeddings, CharacterEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing\n",
    "\n",
    "In here we divide the provided training data into 3 parts; training (80%), validation (10%) and testing (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *Training has ( 14760 ) instances.\n",
      "  *Validation has ( 1845 ) instances.\n",
      "  *TEST has ( 1849 ) instances.\n"
     ]
    }
   ],
   "source": [
    "# reading the raw data\n",
    "raw_data_PATH = 'Dataset/'\n",
    "File_ = open(raw_data_PATH+'ner_dataset.txt')\n",
    "\n",
    "DATA = []\n",
    "sentence = []\n",
    "\n",
    "for line in File_:\n",
    "    try:\n",
    "        if line == '\\n':\n",
    "            DATA.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(line)\n",
    "    except:\n",
    "        print('you have a bad line..',line)\n",
    "\n",
    "random.shuffle(DATA)    # shuffling the data is always good to preven overfitting\n",
    "\n",
    "# dividing the data into trainig (80%), validation (10%) and testing (10%).\n",
    "split_ = int(0.1 * len(DATA))\n",
    "TRAIN_DATA, VAL_DATA, TEST_DATA = DATA[:8*split_], DATA[8*split_:9*split_], DATA[9*split_:]\n",
    "print('  *Training has (',len(TRAIN_DATA),') instances.')\n",
    "print('  *Validation has (',len(VAL_DATA),') instances.')\n",
    "print('  *TEST has (',len(TEST_DATA),') instances.')\n",
    "\n",
    "# print()\n",
    "# storing the data\n",
    "for name,data in zip(['training','validation','test'],[TRAIN_DATA, VAL_DATA, TEST_DATA]):\n",
    "    F = open(raw_data_PATH+'ner_dataset_'+name+'.txt','w')\n",
    "    F.write('\\n'.join([''.join(x) for x in data]))\n",
    "    F.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- spaCy NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for spaCy\n",
    "\n",
    "Inspecting the data, shuffling it and preparing it to be fed into the spaCy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theses functions create training data suitable for the Spacy tool\n",
    "def _reformat_data(data):\n",
    "    for counter, example_ in enumerate(data):\n",
    "        index_ = 0\n",
    "        annotations = {}\n",
    "        sentence, ner_tag = example_\n",
    "        for word, tag in zip(sentence, ner_tag):\n",
    "            #-------------------------------------#\n",
    "            # analysing the NER tag\n",
    "            if '-' in tag:\n",
    "                In, tag = tag.split('-')\n",
    "                if tag not in annotations:\n",
    "                    annotations[tag] = []\n",
    "            else:\n",
    "                In = tag\n",
    "                \n",
    "            #-------------------------------------#\n",
    "            # creating the training data\n",
    "            if In == 'B':\n",
    "                annotations[tag].append([index_, index_+len(word)])\n",
    "            elif In == 'I':\n",
    "                annotations[tag][-1][1] = index_+len(word)\n",
    "            elif In != 'O':\n",
    "                print('=====!!!!!', In)\n",
    "                \n",
    "            index_ += len(word) + 1\n",
    "        \n",
    "        # fix the format\n",
    "        ann = {'entities':[ (val[0],val[1],key) for key in annotations for val in annotations[key]]}\n",
    "            \n",
    "        ## update the training data to fit spacy format\n",
    "        text = ' '.join(sentence)\n",
    "        data[counter] = (text, ann)\n",
    "    return data\n",
    "\n",
    "def _create_training_data(raw_data):\n",
    "    File_ = open(raw_data, 'r')\n",
    "    TRAIN_DATA = []\n",
    "    sentence = []\n",
    "    ner_tag = []\n",
    "\n",
    "    for line in File_:\n",
    "        try:\n",
    "            line = line.split('\\n')[0]\n",
    "\n",
    "            if line == '':\n",
    "                TRAIN_DATA.append([sentence,ner_tag])\n",
    "                sentence = []\n",
    "                ner_tag = []\n",
    "            else:\n",
    "                word, POS1, CNK2, tag = line.split(' ')\n",
    "                sentence.append(word)\n",
    "                ner_tag.append(tag)\n",
    "        except:\n",
    "            print('you have a bad line..',line)\n",
    "            \n",
    "    File_.close()\n",
    "    return _reformat_data(TRAIN_DATA)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 14759 training instances.\n",
      "We have 4 classes in this dataset. {'PER', 'MISC', 'ORG', 'LOC'}\n",
      "  -class( PER ) has 6663 instances.\n",
      "  -class( MISC ) has 3466 instances.\n",
      "  -class( ORG ) has 6101 instances.\n",
      "  -class( LOC ) has 7153 instances.\n"
     ]
    }
   ],
   "source": [
    "# reading the raw data\n",
    "raw_data_PATH = 'Dataset/'\n",
    "\n",
    "# prepare the training data into spacy format\n",
    "TRAIN_DATA = _create_training_data(raw_data_PATH+'ner_dataset_training.txt')\n",
    "VAL_DATA = _create_training_data(raw_data_PATH+'ner_dataset_validation.txt')\n",
    "TEST_DATA = _create_training_data(raw_data_PATH+'ner_dataset_test.txt')\n",
    "\n",
    "# Inspecting the data        \n",
    "print('We have a total of',len(TRAIN_DATA),'training instances.')\n",
    "\n",
    "# print class analysis\n",
    "all_tags = [ent[2] for data in TRAIN_DATA for ent in data[1]['entities']]\n",
    "classes = set(all_tags)\n",
    "print('We have',len(classes),'classes in this dataset.',classes)\n",
    "\n",
    "for c_ in classes:\n",
    "    print('  -class(',c_,') has',all_tags.count(c_),'instances.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the model, Created a blank 'en' model isntead\n"
     ]
    }
   ],
   "source": [
    "# Load or create a blank English model\n",
    "model = 'Spacy/'\n",
    "output_dir = 'Spacy/'\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "        \n",
    "\n",
    "\"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "if model is not None:\n",
    "    try:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    except:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Could not find the model, Created a blank 'en' model isntead\")\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_texts(texts):\n",
    "    colors = {}\n",
    "    colors['ORG'] = 'orange'\n",
    "    colors['PER'] = '#aa9cfc'\n",
    "    colors['LOC'] = 'green'\n",
    "    colors['MISC'] = 'yellow'\n",
    "    options = {'ents': classes, 'colors': colors}\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        Entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        if len(Entities) > 0:\n",
    "            displacy.render(doc, style='ent', jupyter=True, options=options)\n",
    "        else:\n",
    "            print('no entities detected: ',text)\n",
    "        print('--------------------------')\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set(filepath):\n",
    "    ext = filepath.split('.')[-1]\n",
    "    if ext == 'txt':\n",
    "        VAL_DATA = _create_training_data(filepath) \n",
    "    else:\n",
    "        VAL_DATA = []\n",
    "\n",
    "    TP, FN, FP = 0, 0, 0 # True positives, False negatives, False Positives\n",
    "    for text, ann in VAL_DATA:\n",
    "        doc = nlp(text)\n",
    "        GT = sorted(ann['entities'], key=lambda tup: tup[0])\n",
    "        Entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        Ground_Truth = [(text[a[0]:a[1]], a[0], a[1], a[2]) for a in GT]\n",
    "        \n",
    "        TP += len([value for value in Entities if value in Ground_Truth])\n",
    "        FP += len([value for value in Entities if value not in Ground_Truth])\n",
    "        FN += len([value for value in Ground_Truth if value not in Entities])\n",
    "    Pr, Re = TP/(TP+FP), TP/(TP+FN) ## computing Precision and Recall\n",
    "    print('  -Validation: -precision=%.3f -recall=%.3f -f1 score=%.3f'  % (Pr, Re, 2*(Pr*Re)/(Pr+Re)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_iter = 20 # number of iteration\n",
    "\n",
    "# create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    \n",
    "# add labels to model\n",
    "for ent in classes:\n",
    "    ner.add_label(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "*started trianing..\n",
      "  -Trainnig loss {'ner': 530.0087570839813}\n",
      "  -Validation: -precision=0.734 -recall=0.759 -f1 score=0.746\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 193.98312801231032}\n",
      "  -Validation: -precision=0.839 -recall=0.864 -f1 score=0.851\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 133.6101827629867}\n",
      "  -Validation: -precision=0.866 -recall=0.881 -f1 score=0.873\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 103.8997348449231}\n",
      "  -Validation: -precision=0.890 -recall=0.890 -f1 score=0.890\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 91.7709303233944}\n",
      "  -Validation: -precision=0.891 -recall=0.904 -f1 score=0.897\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 81.74727408357197}\n",
      "  -Validation: -precision=0.902 -recall=0.913 -f1 score=0.907\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 68.8015570272417}\n",
      "  -Validation: -precision=0.900 -recall=0.908 -f1 score=0.904\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 62.71420286399199}\n",
      "  -Validation: -precision=0.898 -recall=0.914 -f1 score=0.906\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 57.6629940125926}\n",
      "  -Validation: -precision=0.906 -recall=0.915 -f1 score=0.910\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 54.57270380550764}\n",
      "  -Validation: -precision=0.910 -recall=0.917 -f1 score=0.914\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 48.66704265217571}\n",
      "  -Validation: -precision=0.911 -recall=0.918 -f1 score=0.915\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 44.578225779350966}\n",
      "  -Validation: -precision=0.911 -recall=0.923 -f1 score=0.917\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 42.861066061118905}\n",
      "  -Validation: -precision=0.912 -recall=0.916 -f1 score=0.914\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 41.48462067043344}\n",
      "  -Validation: -precision=0.917 -recall=0.922 -f1 score=0.919\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 38.99869125455994}\n",
      "  -Validation: -precision=0.917 -recall=0.922 -f1 score=0.920\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 38.97783305905275}\n",
      "  -Validation: -precision=0.913 -recall=0.926 -f1 score=0.919\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 38.67695813199222}\n",
      "  -Validation: -precision=0.915 -recall=0.922 -f1 score=0.918\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 30.99506536496841}\n",
      "  -Validation: -precision=0.917 -recall=0.923 -f1 score=0.920\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 33.3604367028745}\n",
      "  -Validation: -precision=0.913 -recall=0.920 -f1 score=0.917\n",
      "Saved model to Spacy\n",
      "  -Trainnig loss {'ner': 28.658652888653766}\n",
      "  -Validation: -precision=0.919 -recall=0.924 -f1 score=0.922\n",
      "Saved model to Spacy\n"
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    print('*started trianing..')\n",
    "    for itn in range(n_iter):\n",
    "        # shuffle the data (reduce the overfitting of the model)\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "        ## printing training and validation losses\n",
    "        print('  -Trainnig loss', losses)\n",
    "        predict_on_test_set(raw_data_PATH+'ner_dataset_validation.txt')\n",
    "        \n",
    "        # save model to output directory\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " city</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">My name is \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Muhannad\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", and I live in the \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". I work in \n",
       "<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Rolls Royce\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To test the model on a txt file use:\n",
    "# predict_on_test_set(raw_data_PATH+'ner_dataset.txt')\n",
    "\n",
    "# To test the model with a sequence of sentences use:\n",
    "predict_on_texts(['New York city','My name is Muhannad, and I live in the US. I work in Rolls Royce.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">7. \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sarah Thorsett\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " ( \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    U.S.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " ) 4:06.80</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Mirhunisa Komarica\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " , a government refugee official , said : \" We want to vote where we were thrown out from .</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    LINZ\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " , \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Austria\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " 1996-08-28</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Ford China JV\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " posts 77 percent net drop in H1 96 .</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plotting the NER tags.\n",
    "# using display-spacy to show the ner values (showing a sample only).\n",
    "random_examples = [int(random.random()*len(VAL_DATA)) for i in range(4)]\n",
    "texts = [VAL_DATA[ex][0] for ex in random_examples]\n",
    "predict_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair model\n",
    "\n",
    "Flair includes contextual word embeddings to predict the named entites within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'<unk>', b'O', b'B-MISC', b'B-PER', b'I-PER', b'B-LOC', b'B-ORG', b'I-LOC', b'I-ORG', b'I-MISC', b'<START>', b'<STOP>']\n",
      "2018-11-29 10:24:41,638 Evaluation method: F1\n",
      "2018-11-29 10:24:41,640 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:24:41,917 epoch 1 - iter 0/462 - loss 42.15810394\n",
      "2018-11-29 10:24:54,769 epoch 1 - iter 46/462 - loss 8.74935113\n",
      "2018-11-29 10:25:07,774 epoch 1 - iter 92/462 - loss 5.99806832\n",
      "2018-11-29 10:25:21,112 epoch 1 - iter 138/462 - loss 4.81196134\n",
      "2018-11-29 10:25:33,686 epoch 1 - iter 184/462 - loss 4.13121788\n",
      "2018-11-29 10:25:46,115 epoch 1 - iter 230/462 - loss 3.67478945\n",
      "2018-11-29 10:25:58,969 epoch 1 - iter 276/462 - loss 3.35587804\n",
      "2018-11-29 10:26:11,713 epoch 1 - iter 322/462 - loss 3.15919730\n",
      "2018-11-29 10:26:25,144 epoch 1 - iter 368/462 - loss 2.95615985\n",
      "2018-11-29 10:26:37,924 epoch 1 - iter 414/462 - loss 2.79990584\n",
      "2018-11-29 10:26:50,589 epoch 1 - iter 460/462 - loss 2.66503113\n",
      "2018-11-29 10:26:50,866 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:27:24,501 EPOCH 1: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:27:24,502 DEV : f-score 0.8846 - acc 0.8846 - tp 2703 - fp 356 - fn 349 - tn 2703\n",
      "2018-11-29 10:27:24,503 TEST: f-score 0.8735 - acc 0.8736 - tp 2622 - fp 382 - fn 377 - tn 2622\n",
      "2018-11-29 10:27:24,503 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:27:24,680 epoch 2 - iter 0/462 - loss 0.72470015\n",
      "2018-11-29 10:27:31,008 epoch 2 - iter 46/462 - loss 1.34955316\n",
      "2018-11-29 10:27:37,098 epoch 2 - iter 92/462 - loss 1.31813694\n",
      "2018-11-29 10:27:42,748 epoch 2 - iter 138/462 - loss 1.27190755\n",
      "2018-11-29 10:27:48,847 epoch 2 - iter 184/462 - loss 1.25444641\n",
      "2018-11-29 10:27:54,824 epoch 2 - iter 230/462 - loss 1.24686708\n",
      "2018-11-29 10:28:00,835 epoch 2 - iter 276/462 - loss 1.23363783\n",
      "2018-11-29 10:28:07,000 epoch 2 - iter 322/462 - loss 1.22169430\n",
      "2018-11-29 10:28:12,987 epoch 2 - iter 368/462 - loss 1.21254134\n",
      "2018-11-29 10:28:18,745 epoch 2 - iter 414/462 - loss 1.18358971\n",
      "2018-11-29 10:28:24,851 epoch 2 - iter 460/462 - loss 1.16791359\n",
      "2018-11-29 10:28:24,936 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:28:39,389 EPOCH 2: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:28:39,391 DEV : f-score 0.9179 - acc 0.9179 - tp 2812 - fp 263 - fn 240 - tn 2812\n",
      "2018-11-29 10:28:39,392 TEST: f-score 0.9085 - acc 0.9085 - tp 2737 - fp 289 - fn 262 - tn 2737\n",
      "2018-11-29 10:28:39,392 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:28:39,552 epoch 3 - iter 0/462 - loss 0.24359506\n",
      "2018-11-29 10:28:45,432 epoch 3 - iter 46/462 - loss 0.96420422\n",
      "2018-11-29 10:28:51,453 epoch 3 - iter 92/462 - loss 0.93837042\n",
      "2018-11-29 10:28:57,784 epoch 3 - iter 138/462 - loss 0.97574096\n",
      "2018-11-29 10:29:04,128 epoch 3 - iter 184/462 - loss 0.95421678\n",
      "2018-11-29 10:29:10,339 epoch 3 - iter 230/462 - loss 0.96181225\n",
      "2018-11-29 10:29:16,564 epoch 3 - iter 276/462 - loss 0.96605771\n",
      "2018-11-29 10:29:22,185 epoch 3 - iter 322/462 - loss 0.96009904\n",
      "2018-11-29 10:29:28,252 epoch 3 - iter 368/462 - loss 0.96383814\n",
      "2018-11-29 10:29:34,905 epoch 3 - iter 414/462 - loss 0.93839935\n",
      "2018-11-29 10:29:40,863 epoch 3 - iter 460/462 - loss 0.93266553\n",
      "2018-11-29 10:29:40,940 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:29:57,574 EPOCH 3: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:29:57,576 DEV : f-score 0.9347 - acc 0.9347 - tp 2849 - fp 195 - fn 203 - tn 2849\n",
      "2018-11-29 10:29:57,576 TEST: f-score 0.9313 - acc 0.9314 - tp 2782 - fp 193 - fn 217 - tn 2782\n",
      "2018-11-29 10:29:57,577 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:29:57,731 epoch 4 - iter 0/462 - loss 1.11642218\n",
      "2018-11-29 10:30:03,416 epoch 4 - iter 46/462 - loss 0.85130906\n",
      "2018-11-29 10:30:08,372 epoch 4 - iter 92/462 - loss 0.86392372\n",
      "2018-11-29 10:30:13,710 epoch 4 - iter 138/462 - loss 0.84220930\n",
      "2018-11-29 10:30:19,590 epoch 4 - iter 184/462 - loss 0.83908327\n",
      "2018-11-29 10:30:25,594 epoch 4 - iter 230/462 - loss 0.83324382\n",
      "2018-11-29 10:30:31,165 epoch 4 - iter 276/462 - loss 0.82345780\n",
      "2018-11-29 10:30:36,930 epoch 4 - iter 322/462 - loss 0.82133886\n",
      "2018-11-29 10:30:42,726 epoch 4 - iter 368/462 - loss 0.82645352\n",
      "2018-11-29 10:30:48,301 epoch 4 - iter 414/462 - loss 0.81933520\n",
      "2018-11-29 10:30:53,961 epoch 4 - iter 460/462 - loss 0.81574412\n",
      "2018-11-29 10:30:54,012 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:31:09,024 EPOCH 4: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:31:09,026 DEV : f-score 0.9354 - acc 0.9354 - tp 2859 - fp 202 - fn 193 - tn 2859\n",
      "2018-11-29 10:31:09,026 TEST: f-score 0.9314 - acc 0.9314 - tp 2791 - fp 203 - fn 208 - tn 2791\n",
      "2018-11-29 10:31:09,027 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:31:09,165 epoch 5 - iter 0/462 - loss 0.85888016\n",
      "2018-11-29 10:31:15,143 epoch 5 - iter 46/462 - loss 0.77064670\n",
      "2018-11-29 10:31:21,127 epoch 5 - iter 92/462 - loss 0.79717153\n",
      "2018-11-29 10:31:27,210 epoch 5 - iter 138/462 - loss 0.78176129\n",
      "2018-11-29 10:31:32,888 epoch 5 - iter 184/462 - loss 0.75805042\n",
      "2018-11-29 10:31:38,791 epoch 5 - iter 230/462 - loss 0.75634237\n",
      "2018-11-29 10:31:44,952 epoch 5 - iter 276/462 - loss 0.74284315\n",
      "2018-11-29 10:31:50,839 epoch 5 - iter 322/462 - loss 0.73773216\n",
      "2018-11-29 10:31:57,052 epoch 5 - iter 368/462 - loss 0.73904836\n",
      "2018-11-29 10:32:02,824 epoch 5 - iter 414/462 - loss 0.74474718\n",
      "2018-11-29 10:32:08,515 epoch 5 - iter 460/462 - loss 0.74113405\n",
      "2018-11-29 10:32:08,611 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:32:22,918 EPOCH 5: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:32:22,919 DEV : f-score 0.9480 - acc 0.9480 - tp 2883 - fp 147 - fn 169 - tn 2883\n",
      "2018-11-29 10:32:22,920 TEST: f-score 0.9446 - acc 0.9446 - tp 2824 - fp 156 - fn 175 - tn 2824\n",
      "2018-11-29 10:32:22,921 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:32:23,082 epoch 6 - iter 0/462 - loss 0.71343148\n",
      "2018-11-29 10:32:28,610 epoch 6 - iter 46/462 - loss 0.65371971\n",
      "2018-11-29 10:32:34,370 epoch 6 - iter 92/462 - loss 0.67128414\n",
      "2018-11-29 10:32:39,812 epoch 6 - iter 138/462 - loss 0.69132268\n",
      "2018-11-29 10:32:45,488 epoch 6 - iter 184/462 - loss 0.68782902\n",
      "2018-11-29 10:32:51,798 epoch 6 - iter 230/462 - loss 0.69125526\n",
      "2018-11-29 10:32:58,122 epoch 6 - iter 276/462 - loss 0.69285163\n",
      "2018-11-29 10:33:03,911 epoch 6 - iter 322/462 - loss 0.68290807\n",
      "2018-11-29 10:33:09,607 epoch 6 - iter 368/462 - loss 0.68779205\n",
      "2018-11-29 10:33:15,021 epoch 6 - iter 414/462 - loss 0.69724485\n",
      "2018-11-29 10:33:20,434 epoch 6 - iter 460/462 - loss 0.68564158\n",
      "2018-11-29 10:33:20,524 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:33:35,656 EPOCH 6: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:33:35,657 DEV : f-score 0.9454 - acc 0.9454 - tp 2894 - fp 176 - fn 158 - tn 2894\n",
      "2018-11-29 10:33:35,657 TEST: f-score 0.9430 - acc 0.9431 - tp 2833 - fp 176 - fn 166 - tn 2833\n",
      "2018-11-29 10:33:35,658 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:33:35,786 epoch 7 - iter 0/462 - loss 0.23466045\n",
      "2018-11-29 10:33:41,707 epoch 7 - iter 46/462 - loss 0.63291350\n",
      "2018-11-29 10:33:48,147 epoch 7 - iter 92/462 - loss 0.62866194\n",
      "2018-11-29 10:33:54,170 epoch 7 - iter 138/462 - loss 0.63659786\n",
      "2018-11-29 10:33:59,912 epoch 7 - iter 184/462 - loss 0.61393818\n",
      "2018-11-29 10:34:05,548 epoch 7 - iter 230/462 - loss 0.61358140\n",
      "2018-11-29 10:34:11,172 epoch 7 - iter 276/462 - loss 0.62838842\n",
      "2018-11-29 10:34:16,918 epoch 7 - iter 322/462 - loss 0.63128443\n",
      "2018-11-29 10:34:22,562 epoch 7 - iter 368/462 - loss 0.62639458\n",
      "2018-11-29 10:34:28,205 epoch 7 - iter 414/462 - loss 0.62799604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 10:34:34,397 epoch 7 - iter 460/462 - loss 0.63046413\n",
      "2018-11-29 10:34:34,486 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:34:48,858 EPOCH 7: lr 0.1000 - bad epochs 1\n",
      "2018-11-29 10:34:48,860 DEV : f-score 0.9519 - acc 0.9519 - tp 2892 - fp 132 - fn 160 - tn 2892\n",
      "2018-11-29 10:34:48,860 TEST: f-score 0.9472 - acc 0.9473 - tp 2830 - fp 146 - fn 169 - tn 2830\n",
      "2018-11-29 10:34:48,862 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:34:49,014 epoch 8 - iter 0/462 - loss 0.32683650\n",
      "2018-11-29 10:34:54,793 epoch 8 - iter 46/462 - loss 0.62219968\n",
      "2018-11-29 10:35:00,343 epoch 8 - iter 92/462 - loss 0.62487941\n",
      "2018-11-29 10:35:06,147 epoch 8 - iter 138/462 - loss 0.61804323\n",
      "2018-11-29 10:35:11,162 epoch 8 - iter 184/462 - loss 0.61333458\n",
      "2018-11-29 10:35:16,983 epoch 8 - iter 230/462 - loss 0.61726546\n",
      "2018-11-29 10:35:23,395 epoch 8 - iter 276/462 - loss 0.61433259\n",
      "2018-11-29 10:35:29,387 epoch 8 - iter 322/462 - loss 0.60106568\n",
      "2018-11-29 10:35:34,921 epoch 8 - iter 368/462 - loss 0.60226531\n",
      "2018-11-29 10:35:40,761 epoch 8 - iter 414/462 - loss 0.59718875\n",
      "2018-11-29 10:35:46,400 epoch 8 - iter 460/462 - loss 0.59115413\n",
      "2018-11-29 10:35:46,497 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:36:01,153 EPOCH 8: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:36:01,155 DEV : f-score 0.9541 - acc 0.9542 - tp 2904 - fp 131 - fn 148 - tn 2904\n",
      "2018-11-29 10:36:01,156 TEST: f-score 0.9506 - acc 0.9506 - tp 2840 - fp 136 - fn 159 - tn 2840\n",
      "2018-11-29 10:36:01,156 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:36:01,309 epoch 9 - iter 0/462 - loss 0.39860371\n",
      "2018-11-29 10:36:06,505 epoch 9 - iter 46/462 - loss 0.53198467\n",
      "2018-11-29 10:36:11,771 epoch 9 - iter 92/462 - loss 0.51684111\n",
      "2018-11-29 10:36:16,783 epoch 9 - iter 138/462 - loss 0.52674748\n",
      "2018-11-29 10:36:22,471 epoch 9 - iter 184/462 - loss 0.53286479\n",
      "2018-11-29 10:36:28,580 epoch 9 - iter 230/462 - loss 0.56076653\n",
      "2018-11-29 10:36:33,571 epoch 9 - iter 276/462 - loss 0.55376842\n",
      "2018-11-29 10:36:39,163 epoch 9 - iter 322/462 - loss 0.55660361\n",
      "2018-11-29 10:36:44,982 epoch 9 - iter 368/462 - loss 0.55995310\n",
      "2018-11-29 10:36:50,719 epoch 9 - iter 414/462 - loss 0.56745685\n",
      "2018-11-29 10:36:56,344 epoch 9 - iter 460/462 - loss 0.56232391\n",
      "2018-11-29 10:36:56,431 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:37:10,746 EPOCH 9: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:37:10,748 DEV : f-score 0.9528 - acc 0.9529 - tp 2900 - fp 135 - fn 152 - tn 2900\n",
      "2018-11-29 10:37:10,748 TEST: f-score 0.9533 - acc 0.9533 - tp 2848 - fp 128 - fn 151 - tn 2848\n",
      "2018-11-29 10:37:10,749 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:37:10,905 epoch 10 - iter 0/462 - loss 0.72902346\n",
      "2018-11-29 10:37:16,758 epoch 10 - iter 46/462 - loss 0.57263761\n",
      "2018-11-29 10:37:22,265 epoch 10 - iter 92/462 - loss 0.54003068\n",
      "2018-11-29 10:37:28,063 epoch 10 - iter 138/462 - loss 0.54858403\n",
      "2018-11-29 10:37:33,949 epoch 10 - iter 184/462 - loss 0.55472757\n",
      "2018-11-29 10:37:39,958 epoch 10 - iter 230/462 - loss 0.55479422\n",
      "2018-11-29 10:37:45,585 epoch 10 - iter 276/462 - loss 0.54247576\n",
      "2018-11-29 10:37:51,113 epoch 10 - iter 322/462 - loss 0.54998938\n",
      "2018-11-29 10:37:56,574 epoch 10 - iter 368/462 - loss 0.54855326\n",
      "2018-11-29 10:38:02,250 epoch 10 - iter 414/462 - loss 0.54882884\n",
      "2018-11-29 10:38:07,994 epoch 10 - iter 460/462 - loss 0.54612448\n",
      "2018-11-29 10:38:08,058 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:38:22,737 EPOCH 10: lr 0.1000 - bad epochs 1\n",
      "2018-11-29 10:38:22,739 DEV : f-score 0.9551 - acc 0.9551 - tp 2917 - fp 139 - fn 135 - tn 2917\n",
      "2018-11-29 10:38:22,739 TEST: f-score 0.9559 - acc 0.9559 - tp 2870 - fp 136 - fn 129 - tn 2870\n",
      "2018-11-29 10:38:22,740 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:38:22,880 epoch 11 - iter 0/462 - loss 0.54635233\n",
      "2018-11-29 10:38:28,670 epoch 11 - iter 46/462 - loss 0.52445119\n",
      "2018-11-29 10:38:34,406 epoch 11 - iter 92/462 - loss 0.51530145\n",
      "2018-11-29 10:38:40,182 epoch 11 - iter 138/462 - loss 0.51386998\n",
      "2018-11-29 10:38:45,778 epoch 11 - iter 184/462 - loss 0.49875320\n",
      "2018-11-29 10:38:51,156 epoch 11 - iter 230/462 - loss 0.50602951\n",
      "2018-11-29 10:38:57,359 epoch 11 - iter 276/462 - loss 0.51170297\n",
      "2018-11-29 10:39:03,266 epoch 11 - iter 322/462 - loss 0.50874605\n",
      "2018-11-29 10:39:08,868 epoch 11 - iter 368/462 - loss 0.51530460\n",
      "2018-11-29 10:39:14,481 epoch 11 - iter 414/462 - loss 0.51698601\n",
      "2018-11-29 10:39:20,408 epoch 11 - iter 460/462 - loss 0.51289399\n",
      "2018-11-29 10:39:20,481 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:39:34,868 EPOCH 11: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:39:34,870 DEV : f-score 0.9590 - acc 0.9590 - tp 2922 - fp 120 - fn 130 - tn 2922\n",
      "2018-11-29 10:39:34,871 TEST: f-score 0.9537 - acc 0.9537 - tp 2856 - fp 134 - fn 143 - tn 2856\n",
      "2018-11-29 10:39:34,872 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:39:35,033 epoch 12 - iter 0/462 - loss 0.22627580\n",
      "2018-11-29 10:39:40,630 epoch 12 - iter 46/462 - loss 0.46142258\n",
      "2018-11-29 10:39:46,242 epoch 12 - iter 92/462 - loss 0.45114776\n",
      "2018-11-29 10:39:51,604 epoch 12 - iter 138/462 - loss 0.47361387\n",
      "2018-11-29 10:39:57,601 epoch 12 - iter 184/462 - loss 0.47211749\n",
      "2018-11-29 10:40:03,435 epoch 12 - iter 230/462 - loss 0.48609564\n",
      "2018-11-29 10:40:09,325 epoch 12 - iter 276/462 - loss 0.50293474\n",
      "2018-11-29 10:40:15,919 epoch 12 - iter 322/462 - loss 0.49596763\n",
      "2018-11-29 10:40:21,495 epoch 12 - iter 368/462 - loss 0.50153829\n",
      "2018-11-29 10:40:26,402 epoch 12 - iter 414/462 - loss 0.50450079\n",
      "2018-11-29 10:40:31,456 epoch 12 - iter 460/462 - loss 0.49879131\n",
      "2018-11-29 10:40:31,520 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:40:45,855 EPOCH 12: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:40:45,857 DEV : f-score 0.9561 - acc 0.9561 - tp 2918 - fp 134 - fn 134 - tn 2918\n",
      "2018-11-29 10:40:45,857 TEST: f-score 0.9508 - acc 0.9508 - tp 2851 - fp 147 - fn 148 - tn 2851\n",
      "2018-11-29 10:40:45,859 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:40:45,999 epoch 13 - iter 0/462 - loss 0.30317247\n",
      "2018-11-29 10:40:51,775 epoch 13 - iter 46/462 - loss 0.38634845\n",
      "2018-11-29 10:40:57,362 epoch 13 - iter 92/462 - loss 0.43487680\n",
      "2018-11-29 10:41:02,729 epoch 13 - iter 138/462 - loss 0.47422872\n",
      "2018-11-29 10:41:08,217 epoch 13 - iter 184/462 - loss 0.47836520\n",
      "2018-11-29 10:41:13,746 epoch 13 - iter 230/462 - loss 0.47954511\n",
      "2018-11-29 10:41:19,123 epoch 13 - iter 276/462 - loss 0.47499335\n",
      "2018-11-29 10:41:24,909 epoch 13 - iter 322/462 - loss 0.48062318\n",
      "2018-11-29 10:41:30,986 epoch 13 - iter 368/462 - loss 0.47819459\n",
      "2018-11-29 10:41:36,924 epoch 13 - iter 414/462 - loss 0.47665759\n",
      "2018-11-29 10:41:42,586 epoch 13 - iter 460/462 - loss 0.48033666\n",
      "2018-11-29 10:41:42,663 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:41:57,985 EPOCH 13: lr 0.1000 - bad epochs 1\n",
      "2018-11-29 10:41:57,987 DEV : f-score 0.9582 - acc 0.9582 - tp 2925 - fp 128 - fn 127 - tn 2925\n",
      "2018-11-29 10:41:57,987 TEST: f-score 0.9587 - acc 0.9587 - tp 2875 - fp 124 - fn 124 - tn 2875\n",
      "2018-11-29 10:41:57,988 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:41:58,172 epoch 14 - iter 0/462 - loss 0.29908091\n",
      "2018-11-29 10:42:04,483 epoch 14 - iter 46/462 - loss 0.38413098\n",
      "2018-11-29 10:42:10,664 epoch 14 - iter 92/462 - loss 0.43530556\n",
      "2018-11-29 10:42:16,273 epoch 14 - iter 138/462 - loss 0.44141972\n",
      "2018-11-29 10:42:21,748 epoch 14 - iter 184/462 - loss 0.46277725\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 10:42:27,523 epoch 14 - iter 230/462 - loss 0.46271749\n",
      "2018-11-29 10:42:33,613 epoch 14 - iter 276/462 - loss 0.45838888\n",
      "2018-11-29 10:42:39,357 epoch 14 - iter 322/462 - loss 0.45494782\n",
      "2018-11-29 10:42:44,917 epoch 14 - iter 368/462 - loss 0.45402290\n",
      "2018-11-29 10:42:50,523 epoch 14 - iter 414/462 - loss 0.44845991\n",
      "2018-11-29 10:42:56,148 epoch 14 - iter 460/462 - loss 0.45012914\n",
      "2018-11-29 10:42:56,243 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:43:10,931 EPOCH 14: lr 0.1000 - bad epochs 2\n",
      "2018-11-29 10:43:10,933 DEV : f-score 0.9614 - acc 0.9614 - tp 2927 - fp 110 - fn 125 - tn 2927\n",
      "2018-11-29 10:43:10,933 TEST: f-score 0.9547 - acc 0.9547 - tp 2857 - fp 129 - fn 142 - tn 2857\n",
      "2018-11-29 10:43:10,935 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:43:11,074 epoch 15 - iter 0/462 - loss 0.12862249\n",
      "2018-11-29 10:43:16,775 epoch 15 - iter 46/462 - loss 0.50750433\n",
      "2018-11-29 10:43:22,390 epoch 15 - iter 92/462 - loss 0.49851422\n",
      "2018-11-29 10:43:28,197 epoch 15 - iter 138/462 - loss 0.50355358\n",
      "2018-11-29 10:43:33,711 epoch 15 - iter 184/462 - loss 0.48079734\n",
      "2018-11-29 10:43:39,103 epoch 15 - iter 230/462 - loss 0.48115477\n",
      "2018-11-29 10:43:44,739 epoch 15 - iter 276/462 - loss 0.46738949\n",
      "2018-11-29 10:43:50,566 epoch 15 - iter 322/462 - loss 0.46614210\n",
      "2018-11-29 10:43:56,104 epoch 15 - iter 368/462 - loss 0.45607878\n",
      "2018-11-29 10:44:01,955 epoch 15 - iter 414/462 - loss 0.46133077\n",
      "2018-11-29 10:44:07,646 epoch 15 - iter 460/462 - loss 0.45946786\n",
      "2018-11-29 10:44:07,717 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:44:22,068 EPOCH 15: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:44:22,069 DEV : f-score 0.9592 - acc 0.9592 - tp 2927 - fp 124 - fn 125 - tn 2927\n",
      "2018-11-29 10:44:22,070 TEST: f-score 0.9572 - acc 0.9572 - tp 2873 - fp 131 - fn 126 - tn 2873\n",
      "2018-11-29 10:44:22,071 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:44:22,237 epoch 16 - iter 0/462 - loss 0.38521466\n",
      "2018-11-29 10:44:27,954 epoch 16 - iter 46/462 - loss 0.41346858\n",
      "2018-11-29 10:44:33,683 epoch 16 - iter 92/462 - loss 0.42912329\n",
      "2018-11-29 10:44:39,125 epoch 16 - iter 138/462 - loss 0.41198648\n",
      "2018-11-29 10:44:44,954 epoch 16 - iter 184/462 - loss 0.42408186\n",
      "2018-11-29 10:44:50,883 epoch 16 - iter 230/462 - loss 0.43136736\n",
      "2018-11-29 10:44:56,775 epoch 16 - iter 276/462 - loss 0.43691057\n",
      "2018-11-29 10:45:02,546 epoch 16 - iter 322/462 - loss 0.44122891\n",
      "2018-11-29 10:45:08,168 epoch 16 - iter 368/462 - loss 0.44418568\n",
      "2018-11-29 10:45:13,392 epoch 16 - iter 414/462 - loss 0.43718471\n",
      "2018-11-29 10:45:18,997 epoch 16 - iter 460/462 - loss 0.43986355\n",
      "2018-11-29 10:45:19,071 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:45:33,535 EPOCH 16: lr 0.1000 - bad epochs 1\n",
      "2018-11-29 10:45:33,536 DEV : f-score 0.9619 - acc 0.9619 - tp 2930 - fp 110 - fn 122 - tn 2930\n",
      "2018-11-29 10:45:33,537 TEST: f-score 0.9557 - acc 0.9557 - tp 2860 - fp 126 - fn 139 - tn 2860\n",
      "2018-11-29 10:45:33,538 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:45:33,680 epoch 17 - iter 0/462 - loss 0.16044649\n",
      "2018-11-29 10:45:39,358 epoch 17 - iter 46/462 - loss 0.38340490\n",
      "2018-11-29 10:45:44,653 epoch 17 - iter 92/462 - loss 0.39837069\n",
      "2018-11-29 10:45:50,370 epoch 17 - iter 138/462 - loss 0.40830708\n",
      "2018-11-29 10:45:56,038 epoch 17 - iter 184/462 - loss 0.42325692\n",
      "2018-11-29 10:46:01,884 epoch 17 - iter 230/462 - loss 0.42385545\n",
      "2018-11-29 10:46:06,786 epoch 17 - iter 276/462 - loss 0.42425715\n",
      "2018-11-29 10:46:11,591 epoch 17 - iter 322/462 - loss 0.42600701\n",
      "2018-11-29 10:46:16,505 epoch 17 - iter 368/462 - loss 0.42159597\n",
      "2018-11-29 10:46:21,698 epoch 17 - iter 414/462 - loss 0.42769367\n",
      "2018-11-29 10:46:27,753 epoch 17 - iter 460/462 - loss 0.42681369\n",
      "2018-11-29 10:46:27,842 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:46:42,494 EPOCH 17: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:46:42,495 DEV : f-score 0.9623 - acc 0.9624 - tp 2941 - fp 119 - fn 111 - tn 2941\n",
      "2018-11-29 10:46:42,496 TEST: f-score 0.9582 - acc 0.9582 - tp 2877 - fp 129 - fn 122 - tn 2877\n",
      "2018-11-29 10:46:42,497 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:46:42,651 epoch 18 - iter 0/462 - loss 0.18439519\n",
      "2018-11-29 10:46:48,326 epoch 18 - iter 46/462 - loss 0.39201082\n",
      "2018-11-29 10:46:53,976 epoch 18 - iter 92/462 - loss 0.40448060\n",
      "2018-11-29 10:46:59,689 epoch 18 - iter 138/462 - loss 0.40051935\n",
      "2018-11-29 10:47:05,745 epoch 18 - iter 184/462 - loss 0.40072571\n",
      "2018-11-29 10:47:11,316 epoch 18 - iter 230/462 - loss 0.39543187\n",
      "2018-11-29 10:47:16,947 epoch 18 - iter 276/462 - loss 0.40282620\n",
      "2018-11-29 10:47:22,593 epoch 18 - iter 322/462 - loss 0.39980850\n",
      "2018-11-29 10:47:27,430 epoch 18 - iter 368/462 - loss 0.41261677\n",
      "2018-11-29 10:47:33,134 epoch 18 - iter 414/462 - loss 0.41339766\n",
      "2018-11-29 10:47:38,794 epoch 18 - iter 460/462 - loss 0.41926079\n",
      "2018-11-29 10:47:38,863 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:47:53,203 EPOCH 18: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:47:53,205 DEV : f-score 0.9639 - acc 0.9639 - tp 2937 - fp 105 - fn 115 - tn 2937\n",
      "2018-11-29 10:47:53,206 TEST: f-score 0.9548 - acc 0.9548 - tp 2860 - fp 132 - fn 139 - tn 2860\n",
      "2018-11-29 10:47:53,206 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:47:53,395 epoch 19 - iter 0/462 - loss 0.46511841\n",
      "2018-11-29 10:47:59,317 epoch 19 - iter 46/462 - loss 0.36193723\n",
      "2018-11-29 10:48:05,049 epoch 19 - iter 92/462 - loss 0.39408886\n",
      "2018-11-29 10:48:10,745 epoch 19 - iter 138/462 - loss 0.39680019\n",
      "2018-11-29 10:48:16,613 epoch 19 - iter 184/462 - loss 0.40037864\n",
      "2018-11-29 10:48:22,322 epoch 19 - iter 230/462 - loss 0.39847582\n",
      "2018-11-29 10:48:28,026 epoch 19 - iter 276/462 - loss 0.40744027\n",
      "2018-11-29 10:48:33,629 epoch 19 - iter 322/462 - loss 0.40297908\n",
      "2018-11-29 10:48:39,616 epoch 19 - iter 368/462 - loss 0.41476918\n",
      "2018-11-29 10:48:45,107 epoch 19 - iter 414/462 - loss 0.41277767\n",
      "2018-11-29 10:48:50,563 epoch 19 - iter 460/462 - loss 0.41142204\n",
      "2018-11-29 10:48:50,615 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:49:05,048 EPOCH 19: lr 0.1000 - bad epochs 0\n",
      "2018-11-29 10:49:05,049 DEV : f-score 0.9630 - acc 0.9631 - tp 2933 - fp 106 - fn 119 - tn 2933\n",
      "2018-11-29 10:49:05,049 TEST: f-score 0.9567 - acc 0.9567 - tp 2863 - fp 123 - fn 136 - tn 2863\n",
      "2018-11-29 10:49:05,050 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:49:05,202 epoch 20 - iter 0/462 - loss 0.41412029\n",
      "2018-11-29 10:49:10,957 epoch 20 - iter 46/462 - loss 0.40400654\n",
      "2018-11-29 10:49:16,778 epoch 20 - iter 92/462 - loss 0.36240515\n",
      "2018-11-29 10:49:22,381 epoch 20 - iter 138/462 - loss 0.38465303\n",
      "2018-11-29 10:49:28,018 epoch 20 - iter 184/462 - loss 0.39556398\n",
      "2018-11-29 10:49:34,058 epoch 20 - iter 230/462 - loss 0.39539368\n",
      "2018-11-29 10:49:39,439 epoch 20 - iter 276/462 - loss 0.38853311\n",
      "2018-11-29 10:49:45,197 epoch 20 - iter 322/462 - loss 0.39081082\n",
      "2018-11-29 10:49:50,859 epoch 20 - iter 368/462 - loss 0.38862731\n",
      "2018-11-29 10:49:56,391 epoch 20 - iter 414/462 - loss 0.39062146\n",
      "2018-11-29 10:50:02,065 epoch 20 - iter 460/462 - loss 0.38685639\n",
      "2018-11-29 10:50:02,146 ----------------------------------------------------------------------------------------------------\n",
      "2018-11-29 10:50:16,452 EPOCH 20: lr 0.1000 - bad epochs 1\n",
      "2018-11-29 10:50:16,454 DEV : f-score 0.9639 - acc 0.9639 - tp 2936 - fp 104 - fn 116 - tn 2936\n",
      "2018-11-29 10:50:16,455 TEST: f-score 0.9597 - acc 0.9598 - tp 2874 - fp 116 - fn 125 - tn 2874\n",
      "2018-11-29 10:50:16,455 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-11-29 10:50:16,456 TEST: f-score 0.9597 - acc 0.9598 - tp 2874 - fp 116 - fn 125 - tn 2874\n",
      "2018-11-29 10:50:16,456 LOC : f-score 0.9730 - acc 0.9730 - tp 883 - fp 26 - fn 23 - tn 883\n",
      "2018-11-29 10:50:16,456 MISC: f-score 0.9260 - acc 0.9260 - tp 413 - fp 27 - fn 39 - tn 413\n",
      "2018-11-29 10:50:16,457 ORG : f-score 0.9377 - acc 0.9377 - tp 715 - fp 47 - fn 48 - tn 715\n",
      "2018-11-29 10:50:16,457 PER : f-score 0.9823 - acc 0.9824 - tp 863 - fp 16 - fn 15 - tn 863\n"
     ]
    }
   ],
   "source": [
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'cnk', 3:'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = 'Dataset/'\n",
    "\n",
    "# training folder\n",
    "flair_folder = 'Flair/'\n",
    "flair_path = Path(flair_folder)\n",
    "if not flair_path.exists():\n",
    "    flair_path.mkdir()\n",
    "        \n",
    "# retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
    "# 1. get the corpus\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.fetch_column_corpus(data_folder, columns,\n",
    "                                                              train_file='ner_dataset_training.txt',\n",
    "                                                              test_file='ner_dataset_test.txt',\n",
    "                                                              dev_file='ner_dataset_validation.txt')\n",
    "                \n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    # use this if you have a potato PC\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use contextual string embeddings\n",
    "    CharLMEmbeddings('news-forward'),\n",
    "    CharLMEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "    \n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "    \n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer: SequenceTaggerTrainer = SequenceTaggerTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train(flair_folder,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The end!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
