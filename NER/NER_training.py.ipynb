{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will attempt to do the Named Entity Recognition task. I will train two models and demonstrate their results below.\n",
    "\n",
    "The models are:<br>\n",
    "1- spaCy NER Gold Parse system [1]. spaCy is an industrial strength NLP library that is widely used by large corporation to handle similar NLP applications. E.g. it's being used by BBC, Microsoft, and many other big companies.\n",
    "\n",
    "2- Flair embeddings [2]. Contextual String Embeddings for Sequence Labeling is currently the state-of-the-art [3] system in Named Enitiy Recognition task, and the only system outperforming Google's BERT [4] model. More information on Flair can be found on their paper [5].\n",
    "\n",
    "I have selected these two models to demonstrate that I am capable of providing a quick & relaiable solution when needed (spaCy). Also, when time/resource allows, I am capable of providing a siginfacntly better solution that is considered the state-of-the-art in the field of NLP (Flair embeddings).\n",
    "\n",
    "spaCy needs a couple of hours to train a decent model on a potato laptop, while Flair embeddings (with CharLMEmbeddings) can take a couple of days on a proper workstation with GPUs. \n",
    "\n",
    "Requirements to run this code:\n",
    "- python 3.6\n",
    "- spacy '2.0.16'\n",
    "- flair\n",
    "\n",
    "[1] https://spacy.io/<br>\n",
    "[2] https://github.com/zalandoresearch/flair<br>\n",
    "[3] https://github.com/zalandoresearch/flair#comparison-with-state-of-the-art<br>\n",
    "[4] https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html<br>\n",
    "[5] https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy import displacy\n",
    "import json\n",
    "import random\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import SequenceTaggerTrainer\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, CharLMEmbeddings, CharacterEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing\n",
    "\n",
    "In here we divide the provided training data into 3 parts; training (80%), validation (10%) and testing (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *Training has ( 14760 ) instances.\n",
      "  *Validation has ( 1845 ) instances.\n",
      "  *TEST has ( 1849 ) instances.\n"
     ]
    }
   ],
   "source": [
    "# reading the raw data\n",
    "raw_data_PATH = '../NER/Dataset/'\n",
    "File_ = open(raw_data_PATH+'ner_dataset.txt')\n",
    "\n",
    "DATA = []\n",
    "sentence = []\n",
    "\n",
    "for line in File_:\n",
    "    try:\n",
    "        if line == '\\n':\n",
    "            DATA.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(line)\n",
    "    except:\n",
    "        print('you have a bad line..',line)\n",
    "\n",
    "random.shuffle(DATA)    # shuffling the data is always good to preven overfitting\n",
    "\n",
    "# dividing the data into trainig (80%), validation (10%) and testing (10%).\n",
    "split_ = int(0.1 * len(DATA))\n",
    "TRAIN_DATA, VAL_DATA, TEST_DATA = DATA[:8*split_], DATA[8*split_:9*split_], DATA[9*split_:]\n",
    "print('  *Training has (',len(TRAIN_DATA),') instances.')\n",
    "print('  *Validation has (',len(VAL_DATA),') instances.')\n",
    "print('  *TEST has (',len(TEST_DATA),') instances.')\n",
    "\n",
    "# print()\n",
    "# storing the data\n",
    "for name,data in zip(['training','validation','test'],[TRAIN_DATA, VAL_DATA, TEST_DATA]):\n",
    "    F = open(raw_data_PATH+'ner_dataset_'+name+'.txt','w')\n",
    "    F.write('\\n'.join([''.join(x) for x in TRAIN_DATA]))\n",
    "    F.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- spaCy NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for spacy\n",
    "\n",
    "Inspecting the data, shuffeling it and preparing it to be fed into the Spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theses functions create training data suitable for the Spacy tool\n",
    "def _reformat_data(data):\n",
    "    for counter, example_ in enumerate(data):\n",
    "        index_ = 0\n",
    "        annotations = {}\n",
    "        sentence, ner_tag = example_\n",
    "        for word, tag in zip(sentence, ner_tag):\n",
    "            #-------------------------------------#\n",
    "            # analysing the NER tag\n",
    "            if '-' in tag:\n",
    "                In, tag = tag.split('-')\n",
    "                if tag not in annotations:\n",
    "                    annotations[tag] = []\n",
    "            else:\n",
    "                In = tag\n",
    "                \n",
    "            #-------------------------------------#\n",
    "            # creating the training data\n",
    "            if In == 'B':\n",
    "                annotations[tag].append([index_, index_+len(word)])\n",
    "            elif In == 'I':\n",
    "                annotations[tag][-1][1] = index_+len(word)\n",
    "            elif In != 'O':\n",
    "                print('=====!!!!!', In)\n",
    "                \n",
    "            index_ += len(word) + 1\n",
    "        \n",
    "        # fix the format\n",
    "        ann = {'entities':[ (val[0],val[1],key) for key in annotations for val in annotations[key]]}\n",
    "            \n",
    "        ## update the training data to fit spacy format\n",
    "        text = ' '.join(sentence)\n",
    "        data[counter] = (text, ann)\n",
    "    return data\n",
    "\n",
    "def _create_training_data(raw_data):\n",
    "    File_ = open(raw_data, 'r')\n",
    "    TRAIN_DATA = []\n",
    "    sentence = []\n",
    "    ner_tag = []\n",
    "\n",
    "    for line in File_:\n",
    "        try:\n",
    "            line = line.split('\\n')[0]\n",
    "\n",
    "            if line == '':\n",
    "                TRAIN_DATA.append([sentence,ner_tag])\n",
    "                sentence = []\n",
    "                ner_tag = []\n",
    "            else:\n",
    "                word, POS1, CNK2, tag = line.split(' ')\n",
    "                sentence.append(word)\n",
    "                ner_tag.append(tag)\n",
    "        except:\n",
    "            print('you have a bad line..',line)\n",
    "            \n",
    "    File_.close()\n",
    "    return _reformat_data(TRAIN_DATA)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 14759 training instances.\n",
      "We have 4 classes in this dataset. {'LOC', 'PER', 'MISC', 'ORG'}\n",
      "  -class( LOC ) has 7246 instances.\n",
      "  -class( PER ) has 6732 instances.\n",
      "  -class( MISC ) has 3493 instances.\n",
      "  -class( ORG ) has 6075 instances.\n"
     ]
    }
   ],
   "source": [
    "# reading the raw data\n",
    "raw_data_PATH = '../NER/Dataset/'\n",
    "\n",
    "# prepare the training data into spacy format\n",
    "TRAIN_DATA = _create_training_data(raw_data_PATH+'ner_dataset_training.txt')\n",
    "VAL_DATA = _create_training_data(raw_data_PATH+'ner_dataset_validation.txt')\n",
    "TEST_DATA = _create_training_data(raw_data_PATH+'ner_dataset_test.txt')\n",
    "\n",
    "# Inspecting the data        \n",
    "print('We have a total of',len(TRAIN_DATA),'training instances.')\n",
    "\n",
    "# print class analysis\n",
    "all_tags = [ent[2] for data in TRAIN_DATA for ent in data[1]['entities']]\n",
    "classes = set(all_tags)\n",
    "print('We have',len(classes),'classes in this dataset.',classes)\n",
    "\n",
    "for c_ in classes:\n",
    "    print('  -class(',c_,') has',all_tags.count(c_),'instances.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find the model, Created a blank 'en' model isntead\n"
     ]
    }
   ],
   "source": [
    "# Load or create a blank English model\n",
    "model = '../NER/Spacy/'\n",
    "output_dir = '../NER/Spacy/'\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "        \n",
    "\n",
    "\"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "if model is not None:\n",
    "    try:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    except:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Could not find the model, Created a blank 'en' model isntead\")\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_texts(texts):\n",
    "    colors = {}\n",
    "    colors['ORG'] = 'orange'\n",
    "    colors['PER'] = '#aa9cfc'\n",
    "    colors['LOC'] = 'green'\n",
    "    colors['MISC'] = 'yellow'\n",
    "    options = {'ents': classes, 'colors': colors}\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        Entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        if len(Entities) > 0:\n",
    "            displacy.render(doc, style='ent', jupyter=True, options=options)\n",
    "        else:\n",
    "            print('no entities detected: ',text)\n",
    "        print('--------------------------')\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set(filepath):\n",
    "    ext = filepath.split('.')[-1]\n",
    "    if ext == 'json':\n",
    "        VAL_DATA = json.load(open(filepath,'r'))\n",
    "    elif ext == 'txt':\n",
    "        VAL_DATA = _create_training_data(filepath) \n",
    "    else:\n",
    "        VAL_DATA = []\n",
    "\n",
    "    TP, FN, FP = 0, 0, 0 # True positives, False negatives, False Positives\n",
    "    for text, ann in VAL_DATA:\n",
    "        doc = nlp(text)\n",
    "        GT = sorted(ann['entities'], key=lambda tup: tup[0])\n",
    "        Entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        Ground_Truth = [(text[a[0]:a[1]], a[0], a[1], a[2]) for a in GT]\n",
    "        \n",
    "        TP += len([value for value in Entities if value in Ground_Truth])\n",
    "        FP += len([value for value in Entities if value not in Ground_Truth])\n",
    "        FN += len([value for value in Ground_Truth if value not in Entities])\n",
    "    Pr, Re = TP/(TP+FP), TP/(TP+FN) ## computing Precision and Recall\n",
    "    print('  -Validation: -precision=%.3f -recall=%.3f -f1 score=%.3f'  % (Pr, Re, 2*(Pr*Re)/(Pr+Re)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_iter = 20 # number of iteration\n",
    "                \n",
    "# create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    \n",
    "# add labels to model\n",
    "for ent in classes:\n",
    "    ner.add_label(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
      "*started trianing..\n",
      "  -Trainnig loss {'ner': 544.528265735968}\n",
      "  -Validation: -precision=0.772 -recall=0.784 -f1 score=0.778\n",
      "Saved model to ../NER/Spacy\n",
      "  -Trainnig loss {'ner': 196.13384832894357}\n",
      "  -Validation: -precision=0.887 -recall=0.903 -f1 score=0.895\n",
      "Saved model to ../NER/Spacy\n",
      "  -Trainnig loss {'ner': 134.60106617566413}\n",
      "  -Validation: -precision=0.926 -recall=0.931 -f1 score=0.929\n",
      "Saved model to ../NER/Spacy\n"
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    print('*started trianing..')\n",
    "    for itn in range(n_iter):\n",
    "        # shuffle the data (reduce the overfitting of the model)\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "        ## printing training and validation losses\n",
    "        print('  -Trainnig loss', losses)\n",
    "        predict_on_test_set(raw_data_PATH+'ner_dataset_validation.txt')\n",
    "        \n",
    "        # save model to output directory\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test the model on a txt file use:\n",
    "# predict_on_test_set(raw_data_PATH+'ner_dataset.txt')\n",
    "\n",
    "# To test the model with a sequence of sentences use:\n",
    "predict_on_texts(['New York city','My name is Muhannad, and I live in the US. I work in Rolls Royce.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the NER tags.\n",
    "# using display-spacy to show the ner values (showing a sample only).\n",
    "random_examples = [int(random.random()*len(VAL_DATA)) for i in range(4)]\n",
    "texts = [VAL_DATA[ex][0] for ex in random_examples]\n",
    "predict_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair model\n",
    "\n",
    "Flair includes word embeddings to predict the named entites within the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'cnk', 3:'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '../NER/Dataset/'\n",
    "\n",
    "# training folder\n",
    "flair_folder = '../NER/Flair/'\n",
    "flair_path = Path(flair_folder)\n",
    "if not flair_path.exists():\n",
    "    flair_path.mkdir()\n",
    "        \n",
    "# retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
    "# 1. get the corpus\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.fetch_column_corpus(data_folder, columns,\n",
    "                                                              train_file='ner_dataset_training.txt',\n",
    "                                                              test_file='ner_dataset_test.txt',\n",
    "                                                              dev_file='ner_dataset_validation.txt')\n",
    "                \n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    # use this if you have a potato PC\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use contextual string embeddings\n",
    "#     CharLMEmbeddings('news-forward'),\n",
    "#     CharLMEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "    \n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "    \n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer: SequenceTaggerTrainer = SequenceTaggerTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train(flair_folder,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
