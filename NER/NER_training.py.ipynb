{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will attempt to do the Named Entity Recognition task. I will train two models and demonstrate their results below.\n",
    "\n",
    "The models are:<br>\n",
    "1- Spacy NER Gold Parse system [1]. Spacy is an industrial strength NLP library that is widely used by large corporation to handle similar NLP applications. E.g. it's being used by BBC, Microsoft, and many other big companies.\n",
    "\n",
    "2- Flair embeddings [2]. Contextual String Embeddings for Sequence Labeling is currently the state-of-the-art [3] system in Named Enitiy Recognition task, and the only system outperforming Google's BERT [4] model. More information on Flair can be found on their paper [5].\n",
    "\n",
    "I have selected these two models to demonstrate that I am capable of providing a quick & relaiable solution when needed (I am talking about Spacy here). Also, when time allows, I am capable of providing a siginfacntly better solution that is considered the state-of-the-art in the field (Flair embeddings).\n",
    "\n",
    "Requirements to run this code:\n",
    "- Python 3.6\n",
    "-  Spacy '2.0.16'\n",
    "-  flair\n",
    "\n",
    "[1] https://spacy.io/<br>\n",
    "[2] https://github.com/zalandoresearch/flair<br>\n",
    "[3] https://github.com/zalandoresearch/flair#comparison-with-state-of-the-art<br>\n",
    "[4] <br>\n",
    "[5] https://drive.google.com/file/d/17yVpFA7MmXaQFTe-HDpZuqw9fJlmzg56/view) and their repo (https://github.com/zalandoresearch/flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy import displacy\n",
    "import json\n",
    "import random\n",
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import SequenceTaggerTrainer\n",
    "from flair.data import TaggedCorpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher, NLPTask\n",
    "from flair.embeddings import TokenEmbeddings, WordEmbeddings, StackedEmbeddings, CharLMEmbeddings, CharacterEmbeddings\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing\n",
    "\n",
    "In here we divide the provided training data into 3 parts; training (80%), validation (10%) and testing (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  *Training has ( 14760 ) instances.\n",
      "  *Validation has ( 1845 ) instances.\n",
      "  *TEST has ( 1849 ) instances.\n"
     ]
    }
   ],
   "source": [
    "# reading the raw data\n",
    "raw_data_PATH = '../NER/Dataset/'\n",
    "File_ = open(raw_data_PATH+'ner_dataset.txt')\n",
    "\n",
    "DATA = []\n",
    "sentence = []\n",
    "\n",
    "for line in File_:\n",
    "    try:\n",
    "        if line == '\\n':\n",
    "            DATA.append(sentence)\n",
    "            sentence = []\n",
    "        else:\n",
    "            sentence.append(line)\n",
    "    except:\n",
    "        print('you have a bad line..',line)\n",
    "\n",
    "random.shuffle(DATA)    # shuffling the data is always good to preven overfitting\n",
    "\n",
    "# dividing the data into trainig (80%), validation (10%) and testing (10%).\n",
    "split_ = int(0.1 * len(DATA))\n",
    "TRAIN_DATA, VAL_DATA, TEST_DATA = DATA[:8*split_], DATA[8*split_:9*split_], DATA[9*split_:]\n",
    "print('  *Training has (',len(TRAIN_DATA),') instances.')\n",
    "print('  *Validation has (',len(VAL_DATA),') instances.')\n",
    "print('  *TEST has (',len(TEST_DATA),') instances.')\n",
    "\n",
    "# print()\n",
    "# storing the data\n",
    "for name,data in zip(['training','validation','test'],[TRAIN_DATA, VAL_DATA, TEST_DATA]):\n",
    "    F = open(raw_data_PATH+'ner_dataset_'+name+'.txt','w')\n",
    "    F.write('\\n'.join([''.join(x) for x in TRAIN_DATA]))\n",
    "    F.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Spacy NER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for spacy\n",
    "\n",
    "Inspecting the data, shuffeling it and preparing it to be fed into the Spacy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# theses functions create training data suitable for the Spacy tool\n",
    "def _reformat_data(data):\n",
    "    for counter, example_ in enumerate(data):\n",
    "        index_ = 0\n",
    "        annotations = {}\n",
    "        sentence, ner_tag = example_\n",
    "        for word, tag in zip(sentence, ner_tag):\n",
    "            #-------------------------------------#\n",
    "            # analysing the NER tag\n",
    "            if '-' in tag:\n",
    "                In, tag = tag.split('-')\n",
    "                if tag not in annotations:\n",
    "                    annotations[tag] = []\n",
    "            else:\n",
    "                In = tag\n",
    "                \n",
    "            #-------------------------------------#\n",
    "            # creating the training data\n",
    "            if In == 'B':\n",
    "                annotations[tag].append([index_, index_+len(word)])\n",
    "            elif In == 'I':\n",
    "                annotations[tag][-1][1] = index_+len(word)\n",
    "            elif In != 'O':\n",
    "                print('=====!!!!!', In)\n",
    "                \n",
    "            index_ += len(word) + 1\n",
    "        \n",
    "        # fix the format\n",
    "        ann = {'entities':[ (val[0],val[1],key) for key in annotations for val in annotations[key]]}\n",
    "            \n",
    "        ## update the training data to fit spacy format\n",
    "        text = ' '.join(sentence)\n",
    "        data[counter] = (text, ann)\n",
    "    return data\n",
    "\n",
    "def _create_training_data(raw_data):\n",
    "    File_ = open(raw_data, 'r')\n",
    "    TRAIN_DATA = []\n",
    "    sentence = []\n",
    "    ner_tag = []\n",
    "\n",
    "    for line in File_:\n",
    "        try:\n",
    "            line = line.split('\\n')[0]\n",
    "\n",
    "            if line == '':\n",
    "                TRAIN_DATA.append([sentence,ner_tag])\n",
    "                sentence = []\n",
    "                ner_tag = []\n",
    "            else:\n",
    "                word, POS1, CNK2, tag = line.split(' ')\n",
    "                sentence.append(word)\n",
    "                ner_tag.append(tag)\n",
    "        except:\n",
    "            print('you have a bad line..',line)\n",
    "            \n",
    "    File_.close()\n",
    "    return _reformat_data(TRAIN_DATA)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have a total of 14759 training instances.\n",
      "We have 4 classes in this dataset. {'ORG', 'MISC', 'PER', 'LOC'}\n",
      "  -class( ORG ) has 6151 instances.\n",
      "  -class( MISC ) has 3483 instances.\n",
      "  -class( PER ) has 6787 instances.\n",
      "  -class( LOC ) has 7231 instances.\n"
     ]
    }
   ],
   "source": [
    "# reading the raw data\n",
    "raw_data_PATH = '../NER/Dataset/'\n",
    "\n",
    "# prepare the training data into spacy format\n",
    "TRAIN_DATA = _create_training_data(raw_data_PATH+'ner_dataset_training.txt')\n",
    "VAL_DATA = _create_training_data(raw_data_PATH+'ner_dataset_validation.txt')\n",
    "TEST_DATA = _create_training_data(raw_data_PATH+'ner_dataset_test.txt')\n",
    "\n",
    "# Inspecting the data        \n",
    "print('We have a total of',len(TRAIN_DATA),'training instances.')\n",
    "\n",
    "# print class analysis\n",
    "all_tags = [ent[2] for data in TRAIN_DATA for ent in data[1]['entities']]\n",
    "classes = set(all_tags)\n",
    "print('We have',len(classes),'classes in this dataset.',classes)\n",
    "\n",
    "for c_ in classes:\n",
    "    print('  -class(',c_,') has',all_tags.count(c_),'instances.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "\n",
    "Here I code the validation and training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model '../NER/Spacy/'\n"
     ]
    }
   ],
   "source": [
    "# Load or create a blank English model\n",
    "## create a new model\n",
    "# model = None\n",
    "## load an existing one\n",
    "model = '../NER/Spacy/'\n",
    "output_dir = '../NER/Spacy/'\n",
    "\n",
    "if output_dir is not None:\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "        \n",
    "\n",
    "\"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n",
    "if model is not None:\n",
    "    try:\n",
    "        nlp = spacy.load(model)  # load existing spaCy model\n",
    "        print(\"Loaded model '%s'\" % model)\n",
    "    except:\n",
    "        nlp = spacy.blank('en')  # create blank Language class\n",
    "        print(\"Could not find the model, Created a blank 'en' model isntead\")\n",
    "else:\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    print(\"Created blank 'en' model\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_texts(texts):\n",
    "    colors = {}\n",
    "    colors['ORG'] = 'orange'\n",
    "    colors['PER'] = '#aa9cfc'\n",
    "    colors['LOC'] = 'green'\n",
    "    colors['MISC'] = 'yellow'\n",
    "    options = {'ents': classes, 'colors': colors}\n",
    "\n",
    "    for text in texts:\n",
    "        doc = nlp(text)\n",
    "        Entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        if len(Entities) > 0:\n",
    "            displacy.render(doc, style='ent', jupyter=True, options=options)\n",
    "        else:\n",
    "            print('no entities detected: ',text)\n",
    "        print('--------------------------')\n",
    "        print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_on_test_set(filepath):\n",
    "    ext = filepath.split('.')[-1]\n",
    "    if ext == 'json':\n",
    "        VAL_DATA = json.load(open(filepath,'r'))\n",
    "    elif ext == 'txt':\n",
    "        VAL_DATA = _create_training_data(filepath) \n",
    "    else:\n",
    "        VAL_DATA = []\n",
    "\n",
    "    TP, FN, FP = 0, 0, 0 # True positives, False negatives, False Positives\n",
    "    for text, ann in VAL_DATA:\n",
    "        doc = nlp(text)\n",
    "        GT = sorted(ann['entities'], key=lambda tup: tup[0])\n",
    "        Entities = [(ent.text, ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]\n",
    "        Ground_Truth = [(text[a[0]:a[1]], a[0], a[1], a[2]) for a in GT]\n",
    "        \n",
    "        TP += len([value for value in Entities if value in Ground_Truth])\n",
    "        FP += len([value for value in Entities if value not in Ground_Truth])\n",
    "        FN += len([value for value in Ground_Truth if value not in Entities])\n",
    "    Pr, Re = TP/(TP+FP), TP/(TP+FN) ## computing Precision and Recall\n",
    "    print('  -Validation: -precision=%.3f -recall=%.3f -f1 score=%.3f'  % (Pr, Re, 2*(Pr*Re)/(Pr+Re)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "n_iter = 30 # number of iteration\n",
    "                \n",
    "# create the built-in pipeline components and add them to the pipeline\n",
    "# nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe('ner')\n",
    "    nlp.add_pipe(ner, last=True)\n",
    "# otherwise, get it so we can add labels\n",
    "else:\n",
    "    ner = nlp.get_pipe('ner')\n",
    "    \n",
    "# add labels to model\n",
    "for ent in classes:\n",
    "    ner.add_label(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*started trianing..\n",
      "  -Trainnig loss {'ner': 200.26405868470712}\n",
      "  -Validation: -precision=0.887 -recall=0.891 -f1 score=0.889\n",
      "Saved model to ../NER/Spacy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-3ed7593742cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# dropout - make it harder to memorise data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# callable to update weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m                 losses=losses)\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;31m## printing training and validation losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'  -Trainnig loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, docs, golds, drop, sgd, losses)\u001b[0m\n\u001b[1;32m    421\u001b[0m             \u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_grads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                 \u001b[0msgd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess_gold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs_golds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# get names of other pipes to disable them during training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "    optimizer = nlp.begin_training()\n",
    "    print('*started trianing..')\n",
    "    for itn in range(n_iter):\n",
    "        # shuffle the data (reduce the overfitting of the model)\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        losses = {}\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=compounding(4., 32., 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(\n",
    "                texts,  # batch of texts\n",
    "                annotations,  # batch of annotations\n",
    "                drop=0.5,  # dropout - make it harder to memorise data\n",
    "                sgd=optimizer,  # callable to update weights\n",
    "                losses=losses)\n",
    "        ## printing training and validation losses\n",
    "        print('  -Trainnig loss', losses)\n",
    "        predict_on_test_set(raw_data_PATH+'ner_dataset_validation.txt')\n",
    "        \n",
    "        # save model to output directory\n",
    "        nlp.to_disk(output_dir)\n",
    "        print(\"Saved model to\", output_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " city</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">My name is \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Muhannad\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ", and I live in the \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ". I work in \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Rolls Royce\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To test the model on a txt file use:\n",
    "# predict_on_test_set(raw_data_PATH+'ner_dataset.txt')\n",
    "\n",
    "# To test the model with a sequence of sentences use:\n",
    "predict_on_texts(['New York city','My name is Muhannad, and I live in the US. I work in Rolls Royce.'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">A club statement said : \" The directors of \n",
       "<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Raith Rovers FC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " invited \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Jimmy Thomson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " to relinquish the post of manager and to resume his former position as youth team coach .</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Nor\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " was \n",
       "<mark class=\"entity\" style=\"background: orange; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Implats\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " the only operation to fail output targets .</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">9 - \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Wayne Ferreira\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " ( \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    South Africa\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " ) vs. qualifier</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Downer\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PER</span>\n",
       "</mark>\n",
       " is the first \n",
       "<mark class=\"entity\" style=\"background: yellow; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Australian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " minister to visit \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    China\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " since the new conservative government took office in \n",
       "<mark class=\"entity\" style=\"background: green; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Canberra\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       " in March .</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Plotting the NER tags.\n",
    "# using display-spacy to show the ner values (showing a sample only).\n",
    "random_examples = [int(random.random()*len(VAL_DATA)) for i in range(4)]\n",
    "texts = [VAL_DATA[ex][0] for ex in random_examples]\n",
    "predict_on_texts(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flair model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define columns\n",
    "columns = {0: 'text', 1: 'pos', 2: 'cnk', 3:'ner'}\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '../NER/Dataset/'\n",
    "\n",
    "# training folder\n",
    "flair_folder = '../NER/Flair/'\n",
    "flair_path = Path(flair_folder)\n",
    "if not flair_path.exists():\n",
    "    flair_path.mkdir()\n",
    "        \n",
    "# retrieve corpus using column format, data folder and the names of the train, dev and test files\n",
    "# 1. get the corpus\n",
    "corpus: TaggedCorpus = NLPTaskDataFetcher.fetch_column_corpus(data_folder, columns,\n",
    "                                                              train_file='ner_dataset_training.txt',\n",
    "                                                              test_file='ner_dataset_test.txt',\n",
    "                                                              dev_file='ner_dataset_validation.txt')\n",
    "                \n",
    "# 2. what tag do we want to predict?\n",
    "tag_type = 'ner'\n",
    "\n",
    "# 3. make the tag dictionary from the corpus\n",
    "tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "print(tag_dictionary.idx2item)\n",
    "\n",
    "# 4. initialize embeddings\n",
    "embedding_types: List[TokenEmbeddings] = [\n",
    "\n",
    "    WordEmbeddings('glove'),\n",
    "\n",
    "    # comment in this line to use character embeddings\n",
    "    # CharacterEmbeddings(),\n",
    "\n",
    "    # comment in these lines to use contextual string embeddings\n",
    "#     CharLMEmbeddings('news-forward'),\n",
    "#     CharLMEmbeddings('news-backward'),\n",
    "]\n",
    "\n",
    "embeddings: StackedEmbeddings = StackedEmbeddings(embeddings=embedding_types)\n",
    "    \n",
    "\n",
    "# 5. initialize sequence tagger\n",
    "tagger: SequenceTagger = SequenceTagger(hidden_size=256,\n",
    "                                        embeddings=embeddings,\n",
    "                                        tag_dictionary=tag_dictionary,\n",
    "                                        tag_type=tag_type,\n",
    "                                        use_crf=True)\n",
    "    \n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer: SequenceTaggerTrainer = SequenceTaggerTrainer(tagger, corpus)\n",
    "\n",
    "# 7. start training\n",
    "trainer.train(flair_folder,\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=32,\n",
    "              max_epochs=150)\n",
    "\n",
    "# 8. plot training curves (optional)\n",
    "from flair.visual.training_curves import Plotter\n",
    "plotter = Plotter()\n",
    "plotter.plot_training_curves(flair_folder+'loss.tsv')\n",
    "plotter.plot_weights(flair_folder+'weights.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
